Project aimed at comparing Transformers and RNNs as models of human sentence processing.

We train LMs of both architectures on ENCOW and use these models to generate surprisal values 
for sentences used in multiple human reading experiments. Then we analyse the data by looking 
at how well these surprisal values predict the human reading data. 

The paper is currently under anonymous review. 
