This folder contains all the scripts to create visual features and caption features and put them in
the appropriate format in an h5 file. 

The prep_*.py  files are the main scripts to run which call all other functions and delivers an h5 file with the specified features. 
I did not try to create a catch all preparation file for all datasets because the datasets are too different. 

aud_feat_functions : functions to create the audio features e.g. filterbanks, mfcc etc.
aud_features : main script to create the features and save them in the appropriate file. 
aud_preproc : preprocessing of the audio
dataset.json : file containing some data and metadata on the flickr database. Needed to split the data into the split used by Karpathy et al. and contains the written captions
filters : functions to make the filters for the filterbank features
melfreq : functions to convert hz to mel and vice versa
places_cleanup : cleans up the places database (i.e. there are images without captions and empty speech files etc. it's a mess)
prep_coco : prepare the ms coco database, add visual features, raw text and tokenised text
prep_flickr : prepare the flickr database, add visual features, raw text, tokenised text and audio features
prep_places : prepare the places database, add visual features and audio features
text_features : functions to read the database's text captions and tokenise them etc. because of different formats flickr and coco have separate functions. 
visual_features : load pretrained pytorch models and create visual features for images. 

It is important to note that all these functions are made to create a h5 file containing all data for all features. 
The structure is: file -> root -> node_1 -> feature_1(e.g. vgg16)-> caption 1
                                                                 -> caption 2 
                                                                 -> caption n
                                         
                                         -> feature_2(tokens)    -> caption 1
                                                                 -> caption 2
                                                                 -> caption n
                                         -> feature_3(audio)     -> caption 1 
                                                                 -> etc.
                                         -> etc.
 
                               -> node_2 -> feature_1
                                         -> etc.

Furthermore for places and coco the nodes (each node is an individual data sample with an image and its captions) are divided at the rootnode into subgroups because h5 recommends no more than 10 000 subnodes on the same root. 

The names for the nodes are by my convention the name of the image file. This makes it easier to associate the data with the nodes and makes sure a list of image and audio file locations are enough to find all data and add it to the right node (as is the case in flickr). In mscoco the captions in the caption file contain an image id making it easy to associate with the node. 
